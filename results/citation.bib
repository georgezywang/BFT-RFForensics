@InProceedings{Alzahrani-DGTS2018,
    author="Alzahrani, Naif and Bulusu, Nirupama",
    editor="Bushnell, Linda and Poovendran, Radha and Ba{\c{s}}ar, Tamer",
    title="Towards True Decentralization: A Blockchain Consensus Protocol Based on Game Theory and Randomness",
    booktitle="Decision and Game Theory for Security",
    year="2018",
    publisher="Springer International Publishing",
    address="Cham",
    pages="465--485",
    abstract="One of the fundamental characteristics of blockchain technology is the consensus protocol. Most of the current consensus protocols are PoW (Proof of Work) based, or fixed-validators based. Nevertheless, PoW requires massive computational effort, which results in high energy and computing resources consumption. Alternatively, fixed-validators protocols rely on fixed, static validators responsible for validating all newly proposed blocks, which opens the door for adversaries to launch several attacks on these validators such as DDoS and eclipse attacks. In this paper, we propose a truly decentralized consensus protocol that does not require PoW and randomly employs a different set of different size of validators on each block's proposal. Additionally, our protocol utilizes a game theoretical model to enforce the honest validators' behavior by rewarding honest validators and penalizing dishonest ones. We have analyzed our protocol and shown that it mitigates various attacks that current protocols suffer from.",
    isbn="978-3-030-01554-1"
}

@InProceedings{Kiayias-CRYPTO2017,
    author="Kiayias, Aggelos and Russell, Alexander and David, Bernardo and Oliynykov, Roman",
    editor="Katz, Jonathan and Shacham, Hovav",
    title="Ouroboros: A Provably Secure Proof-of-Stake Blockchain Protocol",
    booktitle="Advances in Cryptology -- CRYPTO 2017",
    year="2017",
    publisher="Springer International Publishing",
    address="Cham",
    pages="357--388",
    abstract="We present ``Ouroboros'', the first blockchain protocol based on proof of stake with rigorous security guarantees. We establish security properties for the protocol comparable to those achieved by the bitcoin blockchain protocol. As the protocol provides a ``proof of stake'' blockchain discipline, it offers qualitative efficiency advantages over blockchains based on proof of physical resources (e.g., proof of work). We also present a novel reward mechanism for incentivizing Proof of Stake protocols and we prove that, given this mechanism, honest behavior is an approximate Nash equilibrium, thus neutralizing attacks such as selfish mining.",
    isbn="978-3-319-63688-7"
}

@book{10.5555/1296179,
    author = {Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V.},
    title = {Algorithmic Game Theory},
    year = {2007},
    isbn = {0521872820},
    publisher = {Cambridge University Press},
    address = {USA}
}

@InProceedings{Survey,
    title = {A Survey on Applications of Game Theory in Blockchain},
    author = {Liu, Ziyao and Nguyen, Cong and Wang, Wenbo and Niyato, Dusit and Wang, Ping and Liang, Ying-Chang and Kim, Dong In},
    year = {2019},
    month = {Feb},
    pages = {}
}

@misc{sheng2021bft,
    title={BFT Protocol Forensics}, 
    author={Peiyao Sheng and Gerui Wang and Kartik Nayak and Sreeram Kannan and Pramod Viswanath},
    year={2021},
    eprint={2010.06785},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}

 @misc{medium-2019, 
     title={Consensus Series: PBFT}, 
     url={https://medium.com/thundercore/consensus-series-pbft-3e011e7f3691}, 
     journal={Medium}, 
     publisher={ThunderCore}, 
     author={\_kitchen}, 
     year={2019}, 
     month={Nov}
 } 
 
 @misc{Nash-def,
    author = {Drew Fudenberg and Jean Tirole},
    year = {1991},
    month = {August},
    title = {Game Theory},
    pages = {11}
}

@ARTICLE{Jogunola-ieee2021,  
    author={Jogunola, Olamide and Adebisi, Bamidele and Ikpehai, Augustine and Popoola, Segun I. and Gui, Guan and Gačanin, Haris and Ci, Song},  
    journal={IEEE Internet of Things Journal},   
    title={Consensus Algorithms and Deep Reinforcement Learning in Energy Market: A Review},   
    year={2021},  
    volume={8},  
    number={6},  
    pages={4211-4227},  
    doi={10.1109/JIOT.2020.3032162}
}

@misc{yang2021overview,
      title={An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective}, 
      author={Yaodong Yang and Jun Wang},
      year={2021},
      eprint={2011.00583},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@article{lstm-Hochreiter,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@misc{chu2019multiagent,
      title={Multi-Agent Deep Reinforcement Learning for Large-scale Traffic Signal Control}, 
      author={Tianshu Chu and Jie Wang and Lara Codecà and Zhaojian Li},
      year={2019},
      eprint={1903.04527},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sukhbaatar2016learning,
      title={Learning Multiagent Communication with Backpropagation}, 
      author={Sainbayar Sukhbaatar and Arthur Szlam and Rob Fergus},
      year={2016},
      eprint={1605.07736},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{SIMOES202040,
title = {Multi-agent actor centralized-critic with communication},
journal = {Neurocomputing},
volume = {390},
pages = {40-56},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.01.079},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220301314},
author = {David Simões and Nuno Lau and Luís {Paulo Reis}},
keywords = {Multi-agent systems, Neural networks, Emergent communication, Reinforcement learning, Distributed deep learning},
abstract = {Multiple real-world problems are naturally modeled as cooperative multi-agent systems, ranging from satellite formation to traffic monitoring. These systems require algorithms that can learn successful policies with independent agents that rely solely on local partial-observations of the environment. However, multi-agent environments are more complex, due to their partial-observability and non-stationarity from an agent’s perspective, as well as the structural credit assignment problem and the curse of dimensionality, and achieving coordination in such systems remains a complex challenge. To this end, we propose a multi-agent actor-critic algorithm called Asynchronous Advantage Actor Centralized-Critic with Communication (A3C3). A3C3 uses a centralized critic to estimate a value function, decentralized actors to approximate each agent’s policy function, and decentralized communication networks for each agent to share relevant information with its team. The critic can incorporate additional information, like the environment’s global state, when available, and optimizes the actor networks. The actor networks of an agent’s teammates optimize that agent’s communication network, such that each agent learns to output information that is relevant to the policies of others. A3C3 supports a dynamic amount of agents, noisy communication mediums, and can be horizontally scaled to shorten its learning phase. We evaluate A3C3 in two partially-observable multi-agent suites where agents benefit from communicating local information to each other. A3C3 outperforms state-of-the-art multi-agent algorithms, independent approaches, and centralized controllers with access to all agents’ observations.}
}

@misc{TongHanWang,
  author = {Tonghan, Wang},
  title = {ROMA: Multi-Agent Reinforcement Learning with Emergent Roles},
  year = {2013},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/TonghanWang/ROMA}}
}

@article{DBLP:journals/corr/SunehagLGCZJLSL17,
  author    = {Peter Sunehag and
               Guy Lever and
               Audrunas Gruslys and
               Wojciech Marian Czarnecki and
               Vin{\'{\i}}cius Flores Zambaldi and
               Max Jaderberg and
               Marc Lanctot and
               Nicolas Sonnerat and
               Joel Z. Leibo and
               Karl Tuyls and
               Thore Graepel},
  title     = {Value-Decomposition Networks For Cooperative Multi-Agent Learning},
  journal   = {CoRR},
  volume    = {abs/1706.05296},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.05296},
  archivePrefix = {arXiv},
  eprint    = {1706.05296},
  timestamp = {Mon, 13 Aug 2018 16:46:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SunehagLGCZJLSL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1903-08254,
  author    = {Kate Rakelly and
               Aurick Zhou and
               Deirdre Quillen and
               Chelsea Finn and
               Sergey Levine},
  title     = {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic
               Context Variables},
  journal   = {CoRR},
  volume    = {abs/1903.08254},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.08254},
  archivePrefix = {arXiv},
  eprint    = {1903.08254},
  timestamp = {Mon, 01 Apr 2019 14:07:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-08254.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1802-07245,
  author    = {Abhishek Gupta and
               Russell Mendonca and
               Yuxuan Liu and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Meta-Reinforcement Learning of Structured Exploration Strategies},
  journal   = {CoRR},
  volume    = {abs/1802.07245},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.07245},
  archivePrefix = {arXiv},
  eprint    = {1802.07245},
  timestamp = {Thu, 30 Jul 2020 17:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-07245.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}